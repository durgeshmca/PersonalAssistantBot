{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to add semantic search to your agent's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38786/2741872573.py:4: LangChainBetaWarning: The function `init_embeddings` is in beta. It is actively being worked on, so the API may change.\n",
      "  embedding = init_embeddings(model=\"all-MiniLM-L6-v2\",provider=\"huggingface\")\n",
      "/official/official_docs/PythonLearning/PersonalAssistantBot/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "\n",
    "embedding = init_embeddings(model=\"all-MiniLM-L6-v2\",provider=\"huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: I prefer Italian food (similarity: 0.4203020835282931)\n",
      "Memory: I prefer Italian food (similarity: 0.4203020835282931)\n",
      "Memory: I prefer Italian food (similarity: 0.4203020835282931)\n",
      "Memory: I love pizza (similarity: 0.3255428269829057)\n",
      "Memory: I love pizza (similarity: 0.3255428269829057)\n"
     ]
    }
   ],
   "source": [
    "PG1_CONNECTION = \"postgresql://unicode:unicode@localhost:5432/pa\"\n",
    "# connection_kwargs = {\n",
    "#     \"autocommit\": True,\n",
    "#     \"prepare_threshold\": 0,\n",
    "# }\n",
    "# from psycopg import Connection\n",
    "with PostgresStore.from_conn_string(\n",
    "    conn_string=PG1_CONNECTION,index={\n",
    "        \"embed\": embedding,\n",
    "        \"dims\": 384,\n",
    "        \"fields\": [\"text\"]  # specify which fields to embed. Default is the whole serialized valu\n",
    "    }\n",
    ") as store2:\n",
    "    # store.delete((\"user_123\", \"memories\"), \"1\")\n",
    "    # store.delete((\"user_123\", \"memories\"), \"2\")\n",
    "    # store.delete((\"user_123\", \"memories\"), \"3\")\n",
    "    # store.delete((\"user_123\", \"memories\"), \"3\")\n",
    "    # store.delete((\"user_123\", \"memories\"), \"3\")\n",
    "    store2.setup()#only once for migration\n",
    "    # Store some memories\n",
    "    store2.put((\"user_124\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n",
    "    store2.put((\"user_124\", \"memories\"), \"2\", {\"text\": \"I prefer Italian food\"})\n",
    "    store2.put((\"user_124\", \"memories\"), \"3\", {\"text\": \"I don't like spicy food\"})\n",
    "    store2.put((\"user_124\", \"memories\"), \"3\", {\"text\": \"I am studying econometrics\"})\n",
    "    store2.put((\"user_124\", \"memories\"), \"3\", {\"text\": \"I am a plumber\"})\n",
    "\n",
    "    memories = store2.search((\"user_124\", \"memories\"), query=\"I like food?\", limit=5)\n",
    "\n",
    "    for memory in memories:\n",
    "        print(f'Memory: {memory.value[\"text\"]} (similarity: {memory.score})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using in your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_KJvElhc2dQaLqzEh2s01WGdyb3FYlfxcRUBD8hj91HSB2teStoWH\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.graph import START,StateGraph,MessagesState\n",
    "import os\n",
    "load_dotenv()\n",
    "print(os.getenv('GROQ_API_KEY'))\n",
    "llm = init_chat_model(model=\"llama-3.3-70b-versatile\",model_provider=\"groq\")\n",
    "\n",
    "def chat(state, *, store: BaseStore):\n",
    "    # Search based on user's last message\n",
    "    print(state)\n",
    "    items = store.search(\n",
    "        (\"user_124\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n",
    "    )\n",
    "    print(items)\n",
    "    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n",
    "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n",
    "            *state[\"messages\"],\n",
    "        ]\n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='I am hungry', additional_kwargs={}, response_metadata={}, id='f0ddba5c-9de0-4c3f-ac9a-7255dbfa3142')]}\n",
      "[Item(namespace=['user_124', 'memories'], key='37ee181c-3286-4769-a52c-a85bab78db28', value={'text': \"I'm hungry\"}, created_at='2025-03-18T18:39:58.174821+05:30', updated_at='2025-03-18T18:39:58.684548+05:30', score=0.5685279369354248), Item(namespace=['user_124', 'memories'], key='05f4bb12-3247-44e0-a46d-184996d59803', value={'text': \"I'm hungry\"}, created_at='2025-03-18T18:40:01.107755+05:30', updated_at='2025-03-18T18:40:01.648129+05:30', score=0.5685279369354248)]\n",
      "It seems like you're feeling quite hungry. Would you like some suggestions for a meal or a snack? Or perhaps I can help you find a nearby restaurant or recipe?"
     ]
    }
   ],
   "source": [
    "with PostgresStore.from_conn_string(\n",
    "    conn_string=PG1_CONNECTION,index={\n",
    "        \"embed\": embedding,\n",
    "        \"dims\": 384,\n",
    "        \"fields\": [\"text\"]  # specify which fields to embed. Default is the whole serialized valu\n",
    "    }\n",
    ") as store :\n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(chat)\n",
    "    builder.add_edge(START,\"chat\")\n",
    "    graph = builder.compile(store=store)\n",
    "    for message, metadata in graph.stream(\n",
    "        input={\"messages\": [{\"role\": \"user\", \"content\": \"I am hungry\"}]},\n",
    "        stream_mode=\"messages\",\n",
    "    ):\n",
    "        print(message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using in create_react_agent\n",
    "#### Add semantic search to your tool calling agent by injecting the store in the prompt function. You can also use the store in a tool to let your agent manually store or search for memories.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored memory 19c20d0e-8a1d-486f-b56e-523db28c66fdI'm sorry to hear that you're hungry! How about ordering a pizza? Or would you like some other suggestions?"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import InjectedStore\n",
    "from langgraph.store.base import  BaseStore\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def prepare_messages(state: StateGraph,*, store:BaseStore):\n",
    "    # search based on user's last message\n",
    "\n",
    "    items = store.search(\n",
    "        (\"user_124\", \"memories\"),\n",
    "        query=state[\"messages\"][-1].content,\n",
    "        limit=2\n",
    "        )\n",
    "    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n",
    "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"}\n",
    "    ] + state[\"messages\"]\n",
    "\n",
    "# You can also use the store directly within a tool!\n",
    "def upsert_memory(\n",
    "    content: str,\n",
    "    *,\n",
    "    memory_id: Optional[uuid.UUID] = None,\n",
    "    store: Annotated[BaseStore, InjectedStore],\n",
    "):\n",
    "    \"\"\"Upsert a memory in the database.\"\"\"\n",
    "    # The LLM can use this tool to store a new memory\n",
    "    mem_id = memory_id or uuid.uuid4()\n",
    "    store.put(\n",
    "        (\"user_124\", \"memories\"),\n",
    "        key=str(mem_id),\n",
    "        value={\"text\": content},\n",
    "    )\n",
    "    return f\"Stored memory {mem_id}\"\n",
    "\n",
    "# create agent\n",
    "with PostgresStore.from_conn_string(\n",
    "    conn_string=PG1_CONNECTION,index={\n",
    "        \"embed\": embedding,\n",
    "        \"dims\": 384,\n",
    "        \"fields\": [\"text\"]  # specify which fields to embed. Default is the whole serialized valu\n",
    "    }\n",
    ") as store :\n",
    "    agent = create_react_agent(\n",
    "        init_chat_model(model=\"deepseek-r1-distill-llama-70b\",model_provider=\"groq\"),\n",
    "        tools=[upsert_memory],\n",
    "        # The 'prompt' function is run to prepare the messages for the LLM. It is called\n",
    "        # right before each LLM call\n",
    "        prompt=prepare_messages,\n",
    "        store=store,\n",
    "    )\n",
    "    for message, metadata in agent.stream(\n",
    "    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n",
    "        stream_mode=\"messages\",\n",
    "    ):\n",
    "        print(message.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Usage\n",
    "#### Multi-vector indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://langchain-ai.github.io/langgraph/how-tos/memory/semantic-search/#advanced-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add cross-thread persistence to your graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
